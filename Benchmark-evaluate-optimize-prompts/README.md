# Hands-on: Benchmark, evaluate, and optimize prompts across models

## Description

This repository contains the material for the Hands-on: Benchmark, evaluate, and optimize prompts across models.  

## Overview

This session introduces attendees to our new AI Core services for prompt optimizations and evaluation. 

Get hands-on experience with the prompt editor, registry, and benchmarking tools available from the generative AI hub capability in SAP AI Core. You can compare models, evaluate performance, and optimize prompts. Learn how to build resilient, model-agnostic AI workflows and avoid vendor lock-in.

## Requirements

The exercise is designed to be done in the SAP AI Launchpad. For the Hands-on session it is designed for we have provided an SAP AI Launchpad and a technical user to login. If you want to repeat this tutorial outside of the hands-on session you will require your own AI Launchpad instance.

## Exercises

Let's get started with the exercises!

- [Getting Started](exercises/ex0/)
- [Exercise 1 - Setup](exercises/ex1/)
- [Exercise 2 - Evaluate the original prompt](exercises/ex2/)
- [Exercise 3 - Optimize the original prompt](exercises/ex3/)
- [Exercise 4 - Evaluate the optimized prompt](exercises/ex4/)
  
**IMPORTANT**

## Contributing
Please read the [CONTRIBUTING.md](./CONTRIBUTING.md) to understand the contribution guidelines.

## Code of Conduct
Please read the [SAP Open Source Code of Conduct](https://github.com/SAP-samples/.github/blob/main/CODE_OF_CONDUCT.md).

## How to obtain support

Support for the content in this repository is available during the actual time of the online session for which this content has been designed. Otherwise, you may request support via the [Issues](../../issues) tab.

## License
Copyright (c) 2025 SAP SE or an SAP affiliate company. All rights reserved. This project is licensed under the Apache Software License, version 2.0 except as noted otherwise in the [LICENSE](LICENSES/Apache-2.0.txt) file.
